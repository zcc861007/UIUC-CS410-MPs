{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk41rJiyS95x"
      },
      "source": [
        "# CS410 MP1 -- Getting Familiar with Text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install metapy pytoml"
      ],
      "metadata": {
        "id": "Beo-v24czI_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2dMegHTS950"
      },
      "outputs": [],
      "source": [
        "import metapy  # import the MeTA python bindings\n",
        "# You can tell MeTA to log to stderr so you can get progress output when running long-running function calls.\n",
        "metapy.log_to_stderr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89cMPAuzS951"
      },
      "source": [
        "Let's create a document with some content to experiment on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60A4Keq7S951"
      },
      "outputs": [],
      "source": [
        "doc = metapy.index.Document()\n",
        "doc.content(\"I said that I can't believe that it only costs $19.95!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc.content()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zNfpEUPE3eqT",
        "outputId": "2c5537df-0991-4e42-9991-7c455446b6af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I said that I can't believe that it only costs $19.95!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXzPPT3CS952"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14yzXcA5S952"
      },
      "source": [
        "MeTA provides a stream-based interface for performing document tokenization. Each stream starts off with a Tokenizer object, and in most cases you should use the Unicode standard aware ICUTokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5oAfMk8S952"
      },
      "outputs": [],
      "source": [
        "tok = metapy.analyzers.ICUTokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbkcHxxGS953"
      },
      "source": [
        "Tokenizers operate on raw text and provide an Iterable that spits out the individual text tokens. Let's try running just the ICUTokenizer to see what it does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBC1kxhgS954",
        "outputId": "fefd50ee-3b15-4a09-ed90-b6fab2e55a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'I', 'said', 'that', 'I', \"can't\", 'believe', 'that', 'it', 'only', 'costs', '$', '19.95', '!', '</s>']\n"
          ]
        }
      ],
      "source": [
        "tok.set_content(doc.content()) # this could be any string\n",
        "tokens = [token for token in tok]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1Dz65CjS954"
      },
      "source": [
        "One thing that you likely immediately notice is the insertion of these pseudo-XML looking tags. These are called “sentence boundary tags”. As a side-effect, a default-construted ICUTokenizer discovers the sentences in a document by delimiting them with the sentence boundary tags. Let's try tokenizing a multi-sentence document to see what that looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Bai9fv4S955",
        "outputId": "4b06db83-43e8-4961-a6f4-aa9a9569697c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'I', 'said', 'that', 'I', \"can't\", 'believe', 'that', 'it', 'only', 'costs', '$', '19.95', '!', '</s>', '<s>', 'I', 'could', 'only', 'find', 'it', 'for', 'more', 'than', '$', '30', 'before', '.', '</s>']\n"
          ]
        }
      ],
      "source": [
        "doc.content(\"I said that I can't believe that it only costs $19.95! I could only find it for more than $30 before.\")\n",
        "tok.set_content(doc.content())\n",
        "tokens = [token for token in tok]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXS3MuAES955"
      },
      "source": [
        "Most of the information retrieval techniques you have likely been learning about in this class don't need to concern themselves with finding the boundaries between separate sentences in a document, but later today we'll explore a scenario where this might matter more. Let's pass a flag to the ICUTokenizer constructor to disable sentence boundary tags for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_ofC9ENS955",
        "outputId": "b375cac9-38a5-4b77-b9d5-945be001e8ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'said', 'that', 'I', \"can't\", 'believe', 'that', 'it', 'only', 'costs', '$', '19.95', '!', 'I', 'could', 'only', 'find', 'it', 'for', 'more', 'than', '$', '30', 'before', '.']\n"
          ]
        }
      ],
      "source": [
        "tok = metapy.analyzers.ICUTokenizer(suppress_tags=True)\n",
        "tok.set_content(doc.content())\n",
        "tokens = [token for token in tok]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvi9gcqCS956"
      },
      "source": [
        "As mentioned earlier, MeTA treats tokenization as a streaming process, and that it starts with a tokenizer. It is often beneficial to modify the raw underlying tokens of a document, and thus change its representation. The “intermediate” steps in the tokenization stream are represented with objects called Filters. Each filter consumes the content of a previous filter (or a tokenizer) and modifies the tokens coming out of the stream in some way. Let's start by using a simple filter that can help eliminate a lot of noise that we might encounter when tokenizing web documents: a LengthFilter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yHj5NdFS956",
        "outputId": "1a875dcd-d0ab-43ca-95a6-aa25f343e344"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['said', 'that', \"can't\", 'believe', 'that', 'it', 'only', 'costs', '19.95', 'could', 'only', 'find', 'it', 'for', 'more', 'than', '30', 'before']\n"
          ]
        }
      ],
      "source": [
        "tok = metapy.analyzers.LengthFilter(tok, min=2, max=30)\n",
        "tok.set_content(doc.content())\n",
        "tokens = [token for token in tok]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQMOV5zSS956"
      },
      "source": [
        "Here, we can see that the LengthFilter is consuming our original ICUTokenizer. It modifies the token stream by only emitting tokens that are of a minimum length of 2 and a maximum length of 30. This can get rid of a lot of punctuation tokens, but also excessively long tokens such as URLs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4obHciNXS957"
      },
      "source": [
        "## Stopword removal and stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfDJcG1YS957"
      },
      "source": [
        "Another common trick is to remove stopwords. In MeTA, this is done using a ListFilter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rlO8QfoS957",
        "outputId": "b5661753-f96b-40ef-91db-1619f790941a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-01 21:19:00--  https://raw.githubusercontent.com/meta-toolkit/meta/master/data/lemur-stopwords.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2747 (2.7K) [text/plain]\n",
            "Saving to: ‘lemur-stopwords.txt’\n",
            "\n",
            "\rlemur-stopwords.txt   0%[                    ]       0  --.-KB/s               \rlemur-stopwords.txt 100%[===================>]   2.68K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-09-01 21:19:00 (41.6 MB/s) - ‘lemur-stopwords.txt’ saved [2747/2747]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget -nc https://raw.githubusercontent.com/meta-toolkit/meta/master/data/lemur-stopwords.txt  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqQOqvDwS957"
      },
      "source": [
        "Note: wget is a command to download files from links. Another simpler option is to open a web browser, type the link on the address bar and download the file manually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pt-9-em5S958",
        "outputId": "57e7da6c-6d89-4f2b-f81b-7d0ea52114b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"can't\", 'believe', 'costs', '19.95', 'find', '30']\n"
          ]
        }
      ],
      "source": [
        "tok = metapy.analyzers.ListFilter(tok, \"lemur-stopwords.txt\", metapy.analyzers.ListFilter.Type.Reject)\n",
        "tok.set_content(doc.content())\n",
        "tokens = [token for token in tok]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JetA3wF3S958"
      },
      "source": [
        "Here we've downloaded a common list of stopwords and created a ListFilter to reject any tokens that occur in that list of words. You can see how much of a difference removing stopwords can make on the size of a document's token stream!\n",
        "\n",
        "Another common filter that people use is called a stemmer, or lemmatizer. This kind of filter tries to modify individual tokens in such a way that different inflected forms of a word all reduce to the same representation. This lets you, for example, find documents about a “run” when you search “running” or “runs”. A common stemmer is the Porter2 Stemmer, which MeTA has an implementation of. Let's try it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nqZInZjS958",
        "outputId": "029435c7-da06-44f3-8311-5c397da5eaca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"can't\", 'believ', 'cost', '19.95', 'find', '30']\n"
          ]
        }
      ],
      "source": [
        "tok = metapy.analyzers.Porter2Filter(tok)\n",
        "tok.set_content(doc.content())\n",
        "tokens = [token for token in tok]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FFNQ3xeS958"
      },
      "source": [
        "## N-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjRIuEOJS959"
      },
      "source": [
        "Finally, after you've got the token stream configured the way you'd like, it's time to analyze the document by consuming each token from its token stream and performing some actions based on these tokens. In the simplest case, our action can simply be counting how many times these tokens occur. For clarity, let's switch back to a simpler token stream first. We will write a token stream that tokenizes with ICUTokenizer, and then lowercases each token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgQI9b1zS959",
        "outputId": "c87fd231-9242-43b0-d3b5-9b08fb8fa844"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'said', 'that', 'i', \"can't\", 'believe', 'that', 'it', 'only', 'costs', '$', '19.95', '!', 'i', 'could', 'only', 'find', 'it', 'for', 'more', 'than', '$', '30', 'before', '.']\n"
          ]
        }
      ],
      "source": [
        "tok = metapy.analyzers.ICUTokenizer(suppress_tags=True)\n",
        "tok = metapy.analyzers.LowercaseFilter(tok)\n",
        "tok.set_content(doc.content())\n",
        "tokens = [token for token in tok]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4oG1wmLS959"
      },
      "source": [
        "Now, let's count how often each individual token appears in the stream. This representation is called “bag of words” representation or “unigram word counts”. **In MeTA, classes that consume a token stream and emit a document representation are called Analyzers.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgioeIroS959",
        "outputId": "5c2e8ffd-a89b-452e-fad3-5937a5fbebfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I said that I can't believe that it only costs $19.95! I could only find it for more than $30 before.\n",
            "{\"can't\": 1, 'believe': 1, 'than': 1, '$': 2, 'more': 1, 'could': 1, 'it': 2, '!': 1, 'before': 1, '.': 1, 'costs': 1, '19.95': 1, '30': 1, 'said': 1, 'find': 1, 'i': 3, 'only': 2, 'that': 2, 'for': 1}\n"
          ]
        }
      ],
      "source": [
        "ana = metapy.analyzers.NGramWordAnalyzer(1, tok)\n",
        "print(doc.content())\n",
        "unigrams = ana.analyze(doc)\n",
        "print(unigrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BONbN3wwS959"
      },
      "source": [
        "If you noticed the name of the analyzer, you might have realized that you can count not just individual tokens, but groups of them. “Unigram” means “1-gram”, and we count individual tokens. “Bigram” means “2-gram”, and we count adjacent tokens together as a group. Let's try that now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAd3SzCKS959",
        "outputId": "fb656322-385f-4607-f1d3-0fbc53d444cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('before', '.'): 1, ('it', 'for'): 1, ('than', '$'): 1, ('believe', 'that'): 1, ('that', 'it'): 1, ('19.95', '!'): 1, ('that', 'i'): 1, ('said', 'that'): 1, ('could', 'only'): 1, ('i', 'said'): 1, ('find', 'it'): 1, ('!', 'i'): 1, ('costs', '$'): 1, ('i', 'could'): 1, (\"can't\", 'believe'): 1, ('only', 'find'): 1, ('more', 'than'): 1, ('$', '19.95'): 1, ('$', '30'): 1, ('30', 'before'): 1, ('it', 'only'): 1, ('i', \"can't\"): 1, ('only', 'costs'): 1, ('for', 'more'): 1}\n"
          ]
        }
      ],
      "source": [
        "ana = metapy.analyzers.NGramWordAnalyzer(2, tok)\n",
        "bigrams = ana.analyze(doc)\n",
        "print(bigrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuMog_hDS95-"
      },
      "source": [
        "Now the individual “tokens” we're counting are pairs of tokens. Sometimes looking at n-grams of characters is useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OX7WtMKNS95-",
        "outputId": "14b67c0f-49f2-4bdd-c1ac-13391cd2260c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('e', 'v', 'e', ' '): 1, ('t', ' ', 'o', 'n'): 1, ('f', 'o', 'r', ' '): 1, ('!', ' ', 'I', ' '): 1, ('s', 't', 's', ' '): 1, (' ', 'c', 'o', 'u'): 1, ('9', '.', '9', '5'): 1, ('o', 'n', 'l', 'y'): 2, ('y', ' ', 'f', 'i'): 1, (' ', 'o', 'n', 'l'): 2, ('h', 'a', 't', ' '): 2, ('o', 'r', ' ', 'm'): 1, ('b', 'e', 'l', 'i'): 1, ('d', ' ', 'i', 't'): 1, ('t', ' ', 'I', ' '): 1, ('$', '3', '0', ' '): 1, ('c', 'o', 'u', 'l'): 1, ('o', 'r', 'e', '.'): 1, ('o', 'r', 'e', ' '): 1, ('s', ' ', '$', '1'): 1, ('I', ' ', 'c', 'o'): 1, ('$', '1', '9', '.'): 1, ('e', 'l', 'i', 'e'): 1, ('i', 'n', 'd', ' '): 1, ('n', 'd', ' ', 'i'): 1, ('9', '5', '!', ' '): 1, ('5', '!', ' ', 'I'): 1, ('o', 's', 't', 's'): 1, ('n', ' ', '$', '3'): 1, ('a', 't', ' ', 'i'): 1, ('h', 'a', 'n', ' '): 1, (' ', 'c', 'a', 'n'): 1, ('e', 'f', 'o', 'r'): 1, ('n', \"'\", 't', ' '): 1, ('t', ' ', 'f', 'o'): 1, ('r', 'e', ' ', 't'): 1, ('y', ' ', 'c', 'o'): 1, ('n', 'l', 'y', ' '): 2, (' ', 's', 'a', 'i'): 1, (' ', 'I', ' ', 'c'): 2, ('r', ' ', 'm', 'o'): 1, ('I', ' ', 'c', 'a'): 1, (' ', 'c', 'o', 's'): 1, ('a', 't', ' ', 'I'): 1, (' ', 'b', 'e', 'f'): 1, ('t', ' ', 'b', 'e'): 1, ('v', 'e', ' ', 't'): 1, (' ', 'i', 't', ' '): 2, ('3', '0', ' ', 'b'): 1, ('l', 'y', ' ', 'c'): 1, ('t', 'h', 'a', 't'): 2, ('b', 'e', 'f', 'o'): 1, ('t', ' ', 'i', 't'): 1, ('s', 'a', 'i', 'd'): 1, ('1', '9', '.', '9'): 1, ('a', 'n', ' ', '$'): 1, ('a', 'i', 'd', ' '): 1, ('.', '9', '5', '!'): 1, ('t', 's', ' ', '$'): 1, ('I', ' ', 's', 'a'): 1, ('0', ' ', 'b', 'e'): 1, ('o', 'u', 'l', 'd'): 1, ('u', 'l', 'd', ' '): 1, ('l', 'i', 'e', 'v'): 1, (' ', 'f', 'i', 'n'): 1, ('t', 'h', 'a', 'n'): 1, ('l', 'd', ' ', 'o'): 1, (' ', '$', '1', '9'): 1, (' ', 'm', 'o', 'r'): 1, ('i', 'd', ' ', 't'): 1, (' ', '$', '3', '0'): 1, ('f', 'o', 'r', 'e'): 1, ('c', 'o', 's', 't'): 1, ('e', ' ', 't', 'h'): 2, (' ', 'b', 'e', 'l'): 1, (' ', 'f', 'o', 'r'): 1, ('a', 'n', \"'\", 't'): 1, (' ', 't', 'h', 'a'): 3, (\"'\", 't', ' ', 'b'): 1, ('i', 'e', 'v', 'e'): 1, ('d', ' ', 'o', 'n'): 1, ('m', 'o', 'r', 'e'): 1, ('d', ' ', 't', 'h'): 1, ('i', 't', ' ', 'o'): 1, ('c', 'a', 'n', \"'\"): 1, ('i', 't', ' ', 'f'): 1, ('l', 'y', ' ', 'f'): 1, ('f', 'i', 'n', 'd'): 1}\n"
          ]
        }
      ],
      "source": [
        "tok = metapy.analyzers.CharacterTokenizer()\n",
        "ana = metapy.analyzers.NGramWordAnalyzer(4, tok)\n",
        "fourchar_ngrams = ana.analyze(doc)\n",
        "print(fourchar_ngrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi5qKStVS95-"
      },
      "source": [
        "## POS tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M613i0TES95-"
      },
      "source": [
        "Now, let's explore something a little bit different. MeTA also has a natural language processing (NLP) component, which currently supports two major NLP tasks: part-of-speech tagging and syntactic parsing. POS tagging is a task in NLP that involves identifying a type for each word in a sentence. For example, POS tagging can be used to identify all of the nouns in a sentence, or all of the verbs, or adjectives, or… This is useful as first step towards developing an understanding of the meaning of a particular sentence. MeTA places its POS tagging component in its “sequences” library. Let's play with some sequences first to get an idea of how they work. We'll start of by creating a sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8yg_qdqS95-"
      },
      "outputs": [],
      "source": [
        "seq = metapy.sequence.Sequence()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nZtj9fNS95-"
      },
      "source": [
        "Now, we can add individual words to this sequence. Sequences consist of a list of Observations, which are essentially (word, tag) pairs. If we don't yet know the tags for a Sequence, we can just add individual words and leave the tags unset. Words are called “symbols” in the library terminology."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd29qJcgS95-",
        "outputId": "458f5d95-83f0-455d-b651-adaaa60057e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(I, ???), (said, ???), (The, ???), (dog, ???), (ran, ???), (across, ???), (the, ???), (park, ???), (., ???)\n"
          ]
        }
      ],
      "source": [
        "for word in [\"The\", \"dog\", \"ran\", \"across\", \"the\", \"park\", \".\"]:\n",
        "    seq.add_symbol(word)\n",
        "\n",
        "print(seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH8PfVgGS95_"
      },
      "source": [
        "The printed form of the sequence shows that we do not yet know the tags for each word. Let's fill them in by using a pre-trained POS-tagger model that's distributed with MeTA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bXuxzGgS95_",
        "outputId": "3427850a-5ef7-489b-9ff7-cbcd01e08153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-01 21:42:46--  https://github.com/meta-toolkit/meta/releases/download/v3.0.1/greedy-perceptron-tagger.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/16466317/5becfb4a-07f9-11e7-9984-0b59d0729937?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220901%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220901T214247Z&X-Amz-Expires=300&X-Amz-Signature=6f6321e48ce0f83b863182f062cba6d3eeebcf3ac589481fb4fad5b4d1c5c07a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=16466317&response-content-disposition=attachment%3B%20filename%3Dgreedy-perceptron-tagger.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-09-01 21:42:47--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/16466317/5becfb4a-07f9-11e7-9984-0b59d0729937?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220901%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220901T214247Z&X-Amz-Expires=300&X-Amz-Signature=6f6321e48ce0f83b863182f062cba6d3eeebcf3ac589481fb4fad5b4d1c5c07a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=16466317&response-content-disposition=attachment%3B%20filename%3Dgreedy-perceptron-tagger.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6622528 (6.3M) [application/octet-stream]\n",
            "Saving to: ‘greedy-perceptron-tagger.tar.gz’\n",
            "\n",
            "greedy-perceptron-t 100%[===================>]   6.32M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2022-09-01 21:42:47 (78.8 MB/s) - ‘greedy-perceptron-tagger.tar.gz’ saved [6622528/6622528]\n",
            "\n",
            "perceptron-tagger/\n",
            "perceptron-tagger/feature.mapping.gz\n",
            "perceptron-tagger/label.mapping\n",
            "perceptron-tagger/tagger.model.gz\n"
          ]
        }
      ],
      "source": [
        "! wget -nc https://github.com/meta-toolkit/meta/releases/download/v3.0.1/greedy-perceptron-tagger.tar.gz  \n",
        "! tar xvf greedy-perceptron-tagger.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh20pob0S95_",
        "outputId": "d5a695b2-ff92-42e5-f426-05a19153ca4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " > Loading feature mapping: [================================] 100% ETA 00:00:00 \n"
          ]
        }
      ],
      "source": [
        "tagger = metapy.sequence.PerceptronTagger(\"perceptron-tagger/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagger.tag(seq)\n",
        "print(seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voycEPaWIINb",
        "outputId": "1bd9d69b-129f-47c0-89fd-742486e42b9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(I, PRP), (said, VBD)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LomTAg36S95_"
      },
      "source": [
        "Each tag indicates the type of a word, and this particular tagger was trained to output the tags present in the Penn Treebank tagset. But what if we want to POS-tag a document?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf5LBSwNS95_",
        "outputId": "dca025ab-5163-4022-9ee8-ca03bab84132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'I', 'said', 'that', 'I', 'ca', \"n't\", 'believe', 'that', 'it', 'only', 'costs', '$', '19.95', '!', '</s>']\n"
          ]
        }
      ],
      "source": [
        "doc = metapy.index.Document()\n",
        "doc.content(\"I said that I can't believe that it only costs $19.95!\")\n",
        "tok = metapy.analyzers.ICUTokenizer() # keep sentence boundaries!\n",
        "tok = metapy.analyzers.PennTreebankNormalizer(tok)\n",
        "tok.set_content(doc.content())\n",
        "tokens = [token for token in tok]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BKoQlr-S95_"
      },
      "source": [
        "Now, we will write a function that can take a token stream that contains sentence boundary tags and returns a list of Sequence objects. We will not include the sentence boundary tags in the actual Sequence objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSqVkmY1S95_",
        "outputId": "df6fa2ed-6d73-4d53-fb1e-e2c55d9fc1a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(I, PRP), (said, VBD), (that, IN), (I, PRP), (ca, MD), (n't, RB), (believe, VB), (that, IN), (it, PRP), (only, RB), (costs, VBZ), ($, $), (19.95, CD), (!, .)\n"
          ]
        }
      ],
      "source": [
        "def extract_sequences(tok):\n",
        "    sequences = []\n",
        "    for token in tok:\n",
        "        if token == '<s>':\n",
        "            sequences.append(metapy.sequence.Sequence()) # Add a seq for each sentence \n",
        "        elif token != '</s>':\n",
        "            sequences[-1].add_symbol(token)\n",
        "    return sequences\n",
        "\n",
        "doc = metapy.index.Document()\n",
        "doc.content(\"I said that I can't believe that it only costs $19.95!\")\n",
        "tok.set_content(doc.content())\n",
        "for seq in extract_sequences(tok):\n",
        "    tagger.tag(seq)\n",
        "    print(seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8y4lpYfS96A"
      },
      "source": [
        "## Config.toml file: setting up a pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2GxJDtXS96A"
      },
      "source": [
        "In practice, it is often beneficial to combine multiple feature sets together. We can do this with a MultiAnalyzer. Let's combine unigram words, bigram POS tags, and rewrite rules for our document feature representation. We can certainly do this programmatically, but doing so can become tedious quite quickly. Instead, let's use MeTA's configuration file format to specify our analyzer, which we can then load in one line of code. MeTA uses TOML configuration files for all of its configuration. If you haven't heard of TOML before, don't panic! It's a very simple, readable format. Open a text editor and copy the text below, but be careful not to modify the contents. Save it as config.toml ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "#Add this as a config.toml file to your project directory\n",
        "stop-words = \"lemur-stopwords.txt\"\n",
        "\n",
        "[[analyzers]]\n",
        "method = \"ngram-word\"\n",
        "ngram = 1\n",
        "filter = \"default-unigram-chain\"\n",
        "\n",
        "[[analyzers]]\n",
        "method = \"ngram-pos\"\n",
        "ngram = 2\n",
        "filter = [{type = \"icu-tokenizer\"}, {type = \"ptb-normalizer\"}]\n",
        "crf-prefix = \"crf\"\n",
        "\n",
        "[[analyzers]]\n",
        "method = \"tree\"\n",
        "filter = [{type = \"icu-tokenizer\"}, {type = \"ptb-normalizer\"}]\n",
        "features = [\"subtree\"]\n",
        "tagger = \"perceptron-tagger/\"\n",
        "parser = \"parser/\"\n",
        "```"
      ],
      "metadata": {
        "id": "FjOoF7PoKXch"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyizzLnqS96A"
      },
      "source": [
        "Each [[analyzers]] block defines another analyzer to combine for our feature representation. Since “ngram-word” is such a common analyzer, we have defined some default filter chains that can be used with shortcuts. “default-unigram-chain” is a filter chain suitable for unigram words; “default-chain” is a filter chain suitable for bigram words and above.\n",
        "\n",
        "To run this example, we will need to download some additional MeTA resources:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xiB2U8vS96A",
        "outputId": "a6b78918-92ca-468f-a8a0-04551c07fe32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-01 22:30:17--  https://github.com/meta-toolkit/meta/releases/download/v3.0.2/crf.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/16466317/b80b3710-8518-11e7-8623-e7289e51f6af?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220901%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220901T223017Z&X-Amz-Expires=300&X-Amz-Signature=f03250bc67b48c756c7df36179f3d39dcad3a56892129cb942d59c3d246e5474&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=16466317&response-content-disposition=attachment%3B%20filename%3Dcrf.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-09-01 22:30:17--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/16466317/b80b3710-8518-11e7-8623-e7289e51f6af?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220901%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220901T223017Z&X-Amz-Expires=300&X-Amz-Signature=f03250bc67b48c756c7df36179f3d39dcad3a56892129cb942d59c3d246e5474&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=16466317&response-content-disposition=attachment%3B%20filename%3Dcrf.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7533162 (7.2M) [application/octet-stream]\n",
            "Saving to: ‘crf.tar.gz’\n",
            "\n",
            "crf.tar.gz          100%[===================>]   7.18M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-09-01 22:30:18 (63.6 MB/s) - ‘crf.tar.gz’ saved [7533162/7533162]\n",
            "\n",
            "crf/\n",
            "crf/feature.mapping.gz\n",
            "crf/label.mapping\n",
            "crf/observation_ranges.vector\n",
            "crf/transition_ranges.vector\n",
            "crf/observation_weights.vector\n",
            "crf/observations.vector\n",
            "crf/transition_weights.vector\n",
            "crf/transitions.vector\n"
          ]
        }
      ],
      "source": [
        "! wget -nc https://github.com/meta-toolkit/meta/releases/download/v3.0.2/crf.tar.gz\n",
        "! tar xvf crf.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHwqiir-S96A",
        "outputId": "b6fdcd1f-b320-427b-a310-c052e6d266b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-01 22:30:20--  https://github.com/meta-toolkit/meta/releases/download/v3.0.2/greedy-constituency-parser.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/16466317/b80f41f2-8518-11e7-9079-3ff935f6c151?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220901%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220901T223020Z&X-Amz-Expires=300&X-Amz-Signature=475addb1a1092dfb8c967f1ccd4678db59f0c7dceba4ef61598975af62c7eddc&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=16466317&response-content-disposition=attachment%3B%20filename%3Dgreedy-constituency-parser.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-09-01 22:30:20--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/16466317/b80f41f2-8518-11e7-9079-3ff935f6c151?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220901%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220901T223020Z&X-Amz-Expires=300&X-Amz-Signature=475addb1a1092dfb8c967f1ccd4678db59f0c7dceba4ef61598975af62c7eddc&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=16466317&response-content-disposition=attachment%3B%20filename%3Dgreedy-constituency-parser.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 55347000 (53M) [application/octet-stream]\n",
            "Saving to: ‘greedy-constituency-parser.tar.gz’\n",
            "\n",
            "greedy-constituency 100%[===================>]  52.78M  48.2MB/s    in 1.1s    \n",
            "\n",
            "2022-09-01 22:30:21 (48.2 MB/s) - ‘greedy-constituency-parser.tar.gz’ saved [55347000/55347000]\n",
            "\n",
            "parser/\n",
            "parser/parser.trans.gz\n",
            "parser/parser.model.gz\n"
          ]
        }
      ],
      "source": [
        "! wget -nc https://github.com/meta-toolkit/meta/releases/download/v3.0.2/greedy-constituency-parser.tar.gz\n",
        "! tar xvf greedy-constituency-parser.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MkKTZuPS96A"
      },
      "source": [
        "We can now load an analyzer from this configuration file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmRgr4cFS96B",
        "outputId": "6a175d2f-ede2-4f5b-efdc-690ef24d6f5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " > Loading feature mapping: [================================] 100% ETA 00:00:00 \n",
            " > Loading feature mapping: [================================] 100% ETA 00:00:00 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'VBZ_$': 1, 'VB_IN': 1, 'CD_.': 1, 'subtree-(S (NP) (ADVP) (VP))': 1, 'subtree-(NP ($) (CD))': 1, 'subtree-($)': 1, 'subtree-(CD)': 1, 'PRP_RB': 1, 'subtree-(VP (VB) (SBAR))': 1, 'subtree-(NP (PRP))': 3, 'cost': 1, 'IN_PRP': 2, 'subtree-(VP (MD) (RB) (VP))': 1, 'subtree-(S (NP) (VP) (.))': 1, \"can't\": 1, 'subtree-(.)': 1, 'MD_RB': 1, 'subtree-(IN)': 2, 'subtree-(RB)': 2, 'subtree-(VBD)': 1, 'subtree-(VP (VBZ) (NP))': 1, 'subtree-(MD)': 1, 'believ': 1, 'subtree-(VP (VBD) (SBAR))': 1, 'RB_VBZ': 1, 'subtree-(ROOT (S))': 1, 'PRP_VBD': 1, 'subtree-(VBZ)': 1, 'subtree-(S (NP) (VP))': 1, 'subtree-(ADVP (RB))': 1, 'VBD_IN': 1, 'PRP_MD': 1, 'subtree-(SBAR (IN) (S))': 2, 'RB_VB': 1, 'subtree-(VB)': 1, 'subtree-(PRP)': 3, '$_CD': 1}\n"
          ]
        }
      ],
      "source": [
        "ana = metapy.analyzers.load('config.toml')\n",
        "doc = metapy.index.Document()\n",
        "doc.content(\"I said that I can't believe that it only costs $19.95!\")\n",
        "print(ana.analyze(doc))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trying it out on your own!"
      ],
      "metadata": {
        "id": "RyHrzAcfRePQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's test whether you can do such analysis on your own! Inside this repository, you will find example.py where we ask you to fill in your code. You are required to create a function that tokenizes with ICUTokenizer (without the end/start tags, i.e. use the argument \"suppress_tags=True\"), lowercases, removes words with less than 2 and more than 5 characters, performs stemming and produces trigrams for an input sentence. Once you edit the example.py to fill in the function, you can check whether your submission passed the tests."
      ],
      "metadata": {
        "id": "3c2CAscBjFnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens_lowercase(doc):\n",
        "    #Write a token stream that tokenizes with ICUTokenizer (use the argument \"suppress_tags=True\"), \n",
        "    #lowercases, removes words with less than 2 and more than 5  characters\n",
        "    #performs stemming and creates trigrams (name the final call to ana.analyze as \"trigrams\")\n",
        "    '''Place your code here'''\n",
        "    tok = metapy.analyzers.ICUTokenizer(suppress_tags=True)\n",
        "    tok = metapy.analyzers.LowercaseFilter(tok)\n",
        "    tok = metapy.analyzers.LengthFilter(tok, min=2, max=5)\n",
        "    tok = metapy.analyzers.Porter2Filter(tok)\n",
        "\n",
        "    ana = metapy.analyzers.NGramWordAnalyzer(3, tok)\n",
        "    trigrams = ana.analyze(doc)\n",
        "    \n",
        "    #leave the rest of the code as is\n",
        "    tok.set_content(doc.content())\n",
        "    tokens, counts = [], []\n",
        "    for token, count in trigrams.items():\n",
        "        counts.append(count)\n",
        "        tokens.append(token)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "3pUhBScQRlLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = metapy.index.Document()\n",
        "doc.content(\"I said that I can't believe that it only costs $19.95! I could only find it for more than $30 before.\")\n",
        "print(doc.content()) #you can access the document string with .content()\n",
        "\n",
        "tokens = tokens_lowercase(doc)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCjp1tftRnQT",
        "outputId": "e7d37b1f-3df2-466a-dd45-d146b3041dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I said that I can't believe that it only costs $19.95! I could only find it for more than $30 before.\n",
            "[('that', \"can't\", 'that'), ('onli', 'find', 'it'), ('that', 'it', 'onli'), ('19.95', 'could', 'onli'), (\"can't\", 'that', 'it'), ('could', 'onli', 'find'), ('find', 'it', 'for'), ('said', 'that', \"can't\"), ('for', 'more', 'than'), ('it', 'for', 'more'), ('it', 'onli', 'cost'), ('onli', 'cost', '19.95'), ('cost', '19.95', 'could'), ('more', 'than', '30')]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:anaconda3-unidecode]",
      "language": "python",
      "name": "conda-env-anaconda3-unidecode-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}