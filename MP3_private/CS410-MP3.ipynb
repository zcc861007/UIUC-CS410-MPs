{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MP3: Implementing the PLSA Algorithm\n",
        "## DUE: OCT 23, 2022 at 11:59pm\n",
        "\n",
        "In this MP, you will implement the PLSA algorithm discussed in lectures [9.7](https://www.coursera.org/learn/cs-410/lecture/HKe8K/9-7-probabilistic-latent-semantic-analysis-plsa-part-1) and [9.8](https://www.coursera.org/learn/cs-410/lecture/GJyGG/9-8-probabilistic-latent-semantic-analysis-plsa-part-2) of the Text Mining Coursera course.\n",
        "You are not required to implement the PLSA algorithm with a background model (we will run tests assuming the background model has not been implemented). You are provided with two data sets in the `data` folder: `test.txt` and `dblp.txt`. You can assume that each line is a separate document.\n",
        "\n",
        "The `test.txt` data set contains 1000 lines. Each line is a document. The first 100 documents are labeled with the topic the document belongs to, either 0 (“Seattle”) or 1 (“Chicago”).  Each of the first 100 document is generated by sampling completely from the topic that is labelled (i.e., generated from one topic only). The rest 900 documents are generated from a mixture model of the topic “Seattle” and “Chicago”. Run your code to test if your EM algorithm returns reasonable results.\n",
        "\n",
        "The `DBLP.txt` data set is made up of research paper abstracts from DBLP. Each line is a document. Make sure to test your code on the simpler data set `test.txt` before running it on `DBLP.txt`.\n",
        "\n",
        "You are provided with a code skeleton `plsa.py`. Do not change anything in the `def __init__()` function. But feel free to change anything in the `main()` function to test your code.\n",
        "\n",
        "Some suggested tips:\n",
        "1.\tUsing matrices is strongly encouraged (writing nested `for-loops` for calculation is painful)\n",
        "2.\tPython libraries `numpy` and `scipy` are useful in matrix based calculations"
      ],
      "metadata": {
        "id": "HGHw2UJ2dtmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Writing the PLSA Algorithm:\n",
        "The original PLSA model does not contain a background model. This MP also is based on the original PLSA model, you do not have to worry about the background model. However, lectures are all about PLSA with a background model, so you should not attempt to map the formulas on lecture slides directly to the code. Instead, you would need to adjust the formulas on slides by assuming that there is zero probability that the background model would be chosen.  That is, you should set λ<sub>B</sub> to zero. If you do this, you will see that the E-step would essentially only compute a distribution over k topics for z=1, ..., k, given each word, i.e., p(z=i|w). The M-step would also be simpler as p(Z=B|w) is also zero for all words (due to the fact that λ<sub>B</sub>=0). If you are still confused, please take a look at Section 3 of Chase Geigle's note on EM [2] and the note below.\n",
        "\n",
        "\n",
        "The main data structures involved in the implementation of this EM algorithm are three matrices: \n",
        "1. T (topics by words): this is the set of parameters characterizing topic content that we denoted by &theta;<sub>i</sub>'s. Each element is the probability of a particular word in a particular topic. \n",
        "\n",
        "2. D (documents by topics): this is the set of parameters modeling the coverage of topics in each document, which we denoted by p<sub>ij</sub>'s. Each element is the probability of a particular topic is covered in a particular document. \n",
        "\n",
        "3. Z (hidden variables):  For every document, we need one Z which represents the probability that each word in the document has been generated from a particular topic, so for any document, this is a \"word-by-topic\" matrix, encoding p(Z|w) for a particular document. Z is the matrix that we compute in the E-step (based on matrices T and D, which represent our parameters). Note that we need to compute a different Z for each document, so we need to allocate a matrix Z for every document. If we do so, the M-step is simply to use all these Z matrices together with word counts in each document to re-estimate all the parameters, i.e., updating matrices T and D based on Z. Thus at a high level, this is what's happening in the algorithm: \n",
        "    * T and D are initialized. \n",
        "    * E-step computes all Z's based on T and D. \n",
        "    * M-step uses all Z's to update T and D. \n",
        "    * We iterate until the likelihood doesn't change much when we would use T and D as our output. Note that Zs are also very useful (can you imagine some applications of Zs?)."
      ],
      "metadata": {
        "id": "y9WboO9Sd59S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Resources:\n",
        "[1]\t[Cheng’s note](http://sifaka.cs.uiuc.edu/czhai/pub/em-note.pdf) on the EM algorithm  \n",
        "[2]\t[Chase Geigle’s note](http://times.cs.uiuc.edu/course/598f16/notes/em-algorithm.pdf) on the EM algorithm, which includes a derivation of the EM algorithm (see section 4), and  \n",
        "[3]\t[Qiaozhu Mei’s note](http://times.cs.uiuc.edu/course/598f16/plsa-note.pdf) on the EM algorithm for PLSA, which includes a different derivation of the EM algorithm.\n",
        "\n",
        "\n",
        "THIS MP IS CODING AND DEBUGGING HEAVY! PLEASE DON'T LEAVE IT TILL THE LAST MINUTE!"
      ],
      "metadata": {
        "id": "Vv8IijdieA3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Annotations in this programming\n",
        "\n",
        "Indices: i - doc, j - topic, lw - word\n",
        "\n",
        "term_doc_matrix c(t | d): c_i,lw\n",
        "\n",
        "topic_word_prob p(w | z): theta_j,lw\n",
        "\n",
        "document_topic_prob p(z | d): pi_i,j\n",
        "\n",
        "topic_prob p(z | d, w): z_i,j,lw"
      ],
      "metadata": {
        "id": "sxmMK4WctyVo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Jp1aYqWbdmbg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "workspace = '/content/drive/MyDrive/Courses/CS410/MP 3/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(input_matrix):\n",
        "    \"\"\"\n",
        "    Normalizes the rows of a 2d input_matrix so they sum to 1\n",
        "    \"\"\"\n",
        "\n",
        "    row_sums = input_matrix.sum(axis=1)\n",
        "    try:\n",
        "        assert (np.count_nonzero(row_sums)==np.shape(row_sums)[0]) # no row should sum to zero\n",
        "    except Exception:\n",
        "        raise Exception(\"Error while normalizing. Row(s) sum to zero\")\n",
        "    new_matrix = input_matrix / row_sums[:, np.newaxis]\n",
        "    return new_matrix"
      ],
      "metadata": {
        "id": "efB7jlw_r2GW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Corpus(object):\n",
        "\n",
        "    \"\"\"\n",
        "    A collection of documents.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, documents_path):\n",
        "        \"\"\"\n",
        "        Initialize empty document list.\n",
        "        \"\"\"\n",
        "        self.documents_path = documents_path\n",
        "        self.documents = []\n",
        "        self.number_of_documents = 0        \n",
        "        self.vocabulary = []\n",
        "        self.vocabulary_size = 0        \n",
        "\n",
        "        self.term_doc_matrix = None \n",
        "        self.document_topic_prob = None  # P(z | d)\n",
        "        self.topic_word_prob = None  # P(w | z)\n",
        "        self.topic_prob = None  # P(z | d, w)\n",
        "        self.likelihoods = []        \n",
        "\n",
        "    def build_corpus(self):\n",
        "        \"\"\"\n",
        "        Read document, fill in self.documents, a list of list of word\n",
        "        self.documents = [[\"the\", \"day\", \"is\", \"nice\", \"the\", ...], [], []...]\n",
        "        \n",
        "        Update self.number_of_documents\n",
        "        \"\"\"\n",
        "        # #############################\n",
        "        # your code here\n",
        "        f = open(self.documents_path)\n",
        "\n",
        "        for line in f:\n",
        "          line = line.strip()\n",
        "          line = line.split()\n",
        "          self.documents.append(line)\n",
        "        \n",
        "        self.number_of_documents = len(self.documents)\n",
        "        # #############################\n",
        "        \n",
        "        # pass    # REMOVE THIS\n",
        "\n",
        "    def build_vocabulary(self):\n",
        "        \"\"\"\n",
        "        Construct a list of unique words in the whole corpus. Put it in self.vocabulary\n",
        "        for example: [\"rain\", \"the\", ...]\n",
        "\n",
        "        Update self.vocabulary_size\n",
        "        \"\"\"\n",
        "        # #############################\n",
        "        # your code here\n",
        "        self.vocabulary = list(set([item for sublist in self.documents for item in sublist]))\n",
        "        self.vocabulary_size = len(self.vocabulary)\n",
        "        # #############################\n",
        "        \n",
        "        # pass    # REMOVE THIS\n",
        "\n",
        "    def build_term_doc_matrix(self):\n",
        "        \"\"\"\n",
        "        Construct the term-document matrix where each row represents a document, \n",
        "        and each column represents a vocabulary term.\n",
        "\n",
        "        self.term_doc_matrix[i][j] is the count of term j in document i\n",
        "        \"\"\"\n",
        "        # ############################\n",
        "        # your code here\n",
        "\n",
        "        # c_il, i: doc, lw: word\n",
        "        self.term_doc_matrix = np.zeros(shape=(self.number_of_documents,\n",
        "                                               self.vocabulary_size))\n",
        "        for (i, doc) in enumerate(self.documents):\n",
        "          for (lw, word) in enumerate(self.vocabulary):\n",
        "            self.term_doc_matrix[i,lw] = doc.count(word)\n",
        "        # ############################\n",
        "        \n",
        "        # pass    # REMOVE THIS\n",
        "\n",
        "\n",
        "    def initialize_randomly(self, number_of_topics):\n",
        "        \"\"\"\n",
        "        Randomly initialize the matrices: document_topic_prob and topic_word_prob\n",
        "        which hold the probability distributions for P(z | d) and P(w | z): self.document_topic_prob, and self.topic_word_prob\n",
        "\n",
        "        Don't forget to normalize! \n",
        "        HINT: you will find numpy's random matrix useful [https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.random.html]\n",
        "        \"\"\"\n",
        "        # ############################\n",
        "        # your code here\n",
        "\n",
        "        # pi_i,j, i: doc, j: topic\n",
        "        self.document_topic_prob = np.random.random(size=[self.number_of_documents,\n",
        "                                                          number_of_topics])\n",
        "        self.document_topic_prob = normalize(self.document_topic_prob)\n",
        "\n",
        "        # theta_j,lw, j: topic, lw: word\n",
        "        self.topic_word_prob = np.random.random(size=[number_of_topics,\n",
        "                                                      self.vocabulary_size])\n",
        "        self.topic_word_prob = normalize(self.topic_word_prob)\n",
        "        # ############################\n",
        "\n",
        "        # pass    # REMOVE THIS\n",
        "        \n",
        "\n",
        "    def initialize_uniformly(self, number_of_topics):\n",
        "        \"\"\"\n",
        "        Initializes the matrices: self.document_topic_prob and self.topic_word_prob with a uniform \n",
        "        probability distribution. This is used for testing purposes.\n",
        "\n",
        "        DO NOT CHANGE THIS FUNCTION\n",
        "        \"\"\"\n",
        "        self.document_topic_prob = np.ones((self.number_of_documents, number_of_topics))\n",
        "        self.document_topic_prob = normalize(self.document_topic_prob)\n",
        "\n",
        "        self.topic_word_prob = np.ones((number_of_topics, len(self.vocabulary)))\n",
        "        self.topic_word_prob = normalize(self.topic_word_prob)\n",
        "\n",
        "    def initialize(self, number_of_topics, random=False):\n",
        "        \"\"\" Call the functions to initialize the matrices document_topic_prob and topic_word_prob\n",
        "        \"\"\"\n",
        "        print(\"Initializing...\")\n",
        "\n",
        "        if random:\n",
        "            self.initialize_randomly(number_of_topics)\n",
        "        else:\n",
        "            self.initialize_uniformly(number_of_topics)\n",
        "\n",
        "    def expectation_step(self):\n",
        "        \"\"\" The E-step updates P(z | w, d)\n",
        "        \"\"\"\n",
        "        print(\"E step:\")\n",
        "        \n",
        "        # ############################\n",
        "        # your code here\n",
        "        for i in range(self.number_of_documents):\n",
        "          p_z = self.document_topic_prob[i,:][..., None] * self.topic_word_prob  # (j, 1) * (j, lw)\n",
        "          p_z = normalize(p_z.T).T\n",
        "          self.topic_prob[i,:,:] = p_z\n",
        "        \n",
        "        # topic_prob = []\n",
        "        # for i in range(self.number_of_documents):\n",
        "        #   p_z = self.document_topic_prob[i,:] * self.topic_word_prob.T  # (j,) * (j, lw).T\n",
        "        #   p_z = normalize(p_z)\n",
        "        #   topic_prob.append(p_z)\n",
        "        # self.topic_prob = np.array(topic_prob)  # (i, lw, j)\n",
        "        # ############################\n",
        "\n",
        "        # pass    # REMOVE THIS\n",
        "            \n",
        "\n",
        "    def maximization_step(self, number_of_topics):\n",
        "        \"\"\" The M-step updates P(w | z)\n",
        "        \"\"\"\n",
        "        print(\"M step:\")\n",
        "        \n",
        "        # update P(w | z)\n",
        "        \n",
        "        # ############################\n",
        "        # your code here\n",
        "        for j in range(number_of_topics):\n",
        "          # theta = np.sum(self.term_doc_matrix * self.topic_prob[:,:,j], axis=0)   # (i, lw) * (i, lw, j=c)\n",
        "          theta = np.sum(self.term_doc_matrix * self.topic_prob[:,j,:], axis=0)   # (i, lw) * (i, j=c, lw)\n",
        "          self.topic_word_prob[j,:] = theta\n",
        "        self.topic_word_prob = normalize(self.topic_word_prob)\n",
        "        # ############################\n",
        "        \n",
        "        # update P(z | d)\n",
        "\n",
        "        # ############################\n",
        "        # your code here\n",
        "        for i in range(self.number_of_documents):\n",
        "          # pi = self.term_doc_matrix[i,:] @ self.topic_prob[i,:,:]   # (i=c, lw) @ (i=c, lw, j)\n",
        "          # pi = self.topic_prob[i,:,:] @ self.term_doc_matrix[i,:]  # (i=c, j, lw) @ (i=c, lw)\n",
        "          pi = np.matmul(self.topic_prob[i,:,:], self.term_doc_matrix[i,:])  # (i=c, j, lw) @ (i=c, lw)\n",
        "          self.document_topic_prob[i,:] = pi\n",
        "        self.document_topic_prob = normalize(self.document_topic_prob)\n",
        "        # ############################\n",
        "        \n",
        "        # pass    # REMOVE THIS\n",
        "\n",
        "\n",
        "    def calculate_likelihood(self, number_of_topics):\n",
        "        \"\"\" Calculate the current log-likelihood of the model using\n",
        "        the model's updated probability matrices\n",
        "        \n",
        "        Append the calculated log-likelihood to self.likelihoods\n",
        "\n",
        "        \"\"\"\n",
        "        # ############################\n",
        "        # your code here\n",
        "        # logL = np.sum(self.term_doc_matrix * np.log(self.document_topic_prob @ self.topic_word_prob))  # (i, lw) * ((i, j) @ (j, lw))\n",
        "        logL = np.sum( self.term_doc_matrix * np.log(np.matmul(self.document_topic_prob, self.topic_word_prob)) )  # (i, lw) * ((i, j) @ (j, lw))\n",
        "        self.likelihoods.append(logL)\n",
        "        # ############################\n",
        "        \n",
        "        return logL\n",
        "\n",
        "    def plsa(self, number_of_topics, max_iter, epsilon):\n",
        "\n",
        "        \"\"\"\n",
        "        Model topics.\n",
        "        \"\"\"\n",
        "        print (\"EM iteration begins...\")\n",
        "        \n",
        "        # build term-doc matrix\n",
        "        self.build_term_doc_matrix()\n",
        "        \n",
        "        # Create the counter arrays.\n",
        "        \n",
        "        # P(z | d, w)\n",
        "        # self.topic_prob = np.zeros([self.number_of_documents, number_of_topics, self.vocabulary_size], dtype=np.float)\n",
        "        self.topic_prob = np.zeros([self.number_of_documents, number_of_topics, self.vocabulary_size])\n",
        "\n",
        "        # P(z | d) P(w | z)\n",
        "        self.initialize(number_of_topics, random=True)\n",
        "\n",
        "        # Run the EM algorithm\n",
        "        current_likelihood = 0.0\n",
        "\n",
        "        for iteration in range(max_iter):\n",
        "            print(\"Iteration #\" + str(iteration + 1) + \"...\")\n",
        "\n",
        "            # ############################\n",
        "            # your code here\n",
        "            self.expectation_step()\n",
        "            self.maximization_step(number_of_topics)\n",
        "            current_likelihood = self.calculate_likelihood(number_of_topics)\n",
        "            print('current_likelihood: {:.3f}'.format(current_likelihood))\n",
        "\n",
        "            if iteration == max_iter-1:\n",
        "              print('==============================')\n",
        "              print('EM iterations are done!')\n",
        "              \n",
        "              # print('vocabulary =', self.vocabulary)\n",
        "              print('theta_j,lw =')\n",
        "              # print(self.topic_word_prob.round(3))\n",
        "              print(np.concatenate([np.array(self.vocabulary)[None, ...], self.topic_word_prob.round(3)], axis=0))\n",
        "              \n",
        "              print('pi_i,j =')\n",
        "              print(self.document_topic_prob[:10].round(3))\n",
        "            # ############################\n",
        "\n",
        "            # pass    # REMOVE THIS"
      ],
      "metadata": {
        "id": "eLjEbna2r5Lz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# documents_path = 'data/test.txt'\n",
        "documents_path = workspace + 'data/test.txt'\n",
        "# documents_path = workspace + 'data/DBLP.txt'\n",
        "corpus = Corpus(documents_path)  # instantiate corpus\n",
        "corpus.build_corpus()\n",
        "corpus.build_vocabulary()\n",
        "print(corpus.vocabulary)\n",
        "print(\"Vocabulary size:\" + str(len(corpus.vocabulary)))\n",
        "print(\"Number of documents:\" + str(len(corpus.documents)))\n",
        "number_of_topics = 2\n",
        "max_iterations = 50\n",
        "epsilon = 0.001\n",
        "corpus.plsa(number_of_topics, max_iterations, epsilon)"
      ],
      "metadata": {
        "id": "3cicCdUMsAP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46b855f4-f414-478c-a63f-0d2b8c78f2bc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mount', 'willis', 'chicago', 'seattle', 'rainier', '0', 'tower', '1']\n",
            "Vocabulary size:8\n",
            "Number of documents:1000\n",
            "EM iteration begins...\n",
            "Initializing...\n",
            "Iteration #1...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -179013.308\n",
            "Iteration #2...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -177497.125\n",
            "Iteration #3...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -175463.495\n",
            "Iteration #4...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -172528.876\n",
            "Iteration #5...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -168907.845\n",
            "Iteration #6...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -165501.648\n",
            "Iteration #7...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -163107.678\n",
            "Iteration #8...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -161741.114\n",
            "Iteration #9...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -161006.971\n",
            "Iteration #10...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -160605.161\n",
            "Iteration #11...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -160381.685\n",
            "Iteration #12...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -160249.948\n",
            "Iteration #13...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -160162.716\n",
            "Iteration #14...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -160097.900\n",
            "Iteration #15...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -160046.021\n",
            "Iteration #16...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -160003.350\n",
            "Iteration #17...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159968.593\n",
            "Iteration #18...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159940.521\n",
            "Iteration #19...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159917.443\n",
            "Iteration #20...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159897.825\n",
            "Iteration #21...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159880.691\n",
            "Iteration #22...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159865.517\n",
            "Iteration #23...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159851.858\n",
            "Iteration #24...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159839.294\n",
            "Iteration #25...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159827.537\n",
            "Iteration #26...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159816.499\n",
            "Iteration #27...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159806.259\n",
            "Iteration #28...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159796.922\n",
            "Iteration #29...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159788.554\n",
            "Iteration #30...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159781.033\n",
            "Iteration #31...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159774.130\n",
            "Iteration #32...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159767.630\n",
            "Iteration #33...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159761.470\n",
            "Iteration #34...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159755.870\n",
            "Iteration #35...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159751.087\n",
            "Iteration #36...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159747.106\n",
            "Iteration #37...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159743.756\n",
            "Iteration #38...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159740.892\n",
            "Iteration #39...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159738.396\n",
            "Iteration #40...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159736.146\n",
            "Iteration #41...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159734.036\n",
            "Iteration #42...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159732.046\n",
            "Iteration #43...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159730.196\n",
            "Iteration #44...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159728.475\n",
            "Iteration #45...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159726.833\n",
            "Iteration #46...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159725.225\n",
            "Iteration #47...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159723.616\n",
            "Iteration #48...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159722.006\n",
            "Iteration #49...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159720.431\n",
            "Iteration #50...\n",
            "E step:\n",
            "M step:\n",
            "current_likelihood: -159718.932\n",
            "==============================\n",
            "EM iterations are done!\n",
            "theta_j,lw =\n",
            "[['mount' 'willis' 'chicago' 'seattle' 'rainier' '0' 'tower' '1']\n",
            " ['0.056' '0.298' '0.296' '0.052' '0.002' '0.0' '0.294' '0.001']\n",
            " ['0.299' '0.0' '0.0' '0.303' '0.397' '0.001' '0.0' '0.0']]\n",
            "pi_i,j =\n",
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]]\n"
          ]
        }
      ]
    }
  ]
}